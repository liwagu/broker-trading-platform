# Kronos Model Integration

This package integrates the Kronos time-series prediction model using the **Model Registry pattern** (industry standard for ML operationalization).

## ğŸ—ï¸ Architecture: Model Registry Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Service Container                  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Inference Code (kronos_integration/)          â”‚    â”‚
â”‚  â”‚  - Model architecture                          â”‚    â”‚
â”‚  â”‚  - Preprocessing logic                         â”‚    â”‚
â”‚  â”‚  - Prediction API                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                               â”‚
â”‚                   At Startup Downloads                   â”‚
â”‚                          â†“                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚       Hugging Face Hub (Model Registry)        â”‚    â”‚
â”‚  â”‚  - Model weights (NeoQuasar/Kronos-small)      â”‚    â”‚
â”‚  â”‚  - Tokenizer weights (Kronos-Tokenizer-base)   â”‚    â”‚
â”‚  â”‚  - Cached locally after first download         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Ship only inference code in containers (smaller images)
- âœ… Version control for model weights separate from code
- âœ… Easy model updates without code changes
- âœ… Support for private models via HuggingFace tokens
- âœ… Automatic caching for faster subsequent loads

---

## ğŸ“‹ What's Included

### Inference Code (Shipped in Container)
```
ai-service/kronos_integration/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ predictor.py             # Main prediction logic
â”œâ”€â”€ model/                   # Kronos model architecture
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ kronos.py           # Model & Tokenizer classes
â”‚   â””â”€â”€ module.py           # Neural network modules
â””â”€â”€ README.md               # This file
```

### Model Weights (Downloaded from HuggingFace)
- **Model**: `NeoQuasar/Kronos-small` (~150MB)
- **Tokenizer**: `NeoQuasar/Kronos-Tokenizer-base` (~50MB)
- **Cached at**: `~/.cache/huggingface/` (inside container)

---

## ğŸš€ Usage

### 1. Environment Variables

```bash
# Operating Mode
KRONOS_MODE=auto              # auto | kronos | simulated
                              # auto: Try Kronos, fallback to simulator
                              # kronos: Require Kronos (fail if unavailable)
                              # simulated: Always use simulator

# Device Configuration
KRONOS_DEVICE=cpu             # cpu | cuda | cuda:0

# Model Registry (HuggingFace Hub)
KRONOS_MODEL_ID=NeoQuasar/Kronos-small
KRONOS_TOKENIZER_ID=NeoQuasar/Kronos-Tokenizer-base

# Model Parameters
KRONOS_MAX_CONTEXT=512        # Maximum context length
KRONOS_CLIP=5                 # Clipping value for normalization

# Optional: Private Models
HF_TOKEN=hf_xxxxxxxxxxxxx     # HuggingFace API token
```

### 2. Docker Compose (Recommended)

```yaml
services:
  ai-service:
    build:
      context: ./ai-service
    environment:
      - KRONOS_MODE=auto
      - KRONOS_MODEL_ID=NeoQuasar/Kronos-small
    volumes:
      # Optional: Persist model cache across restarts
      - ./cache/huggingface:/root/.cache/huggingface
    ports:
      - "5001:5001"
```

```bash
docker-compose up --build
```

### 3. Local Development

```bash
cd ai-service

# Install dependencies
pip install -r requirements.txt

# Run with Kronos model
KRONOS_MODE=kronos python main.py

# Run with simulator (no model download)
KRONOS_MODE=simulated python main.py
```

---

## ğŸ”‘ HuggingFace Setup

### Option A: Use Public Pre-trained Models (Easiest)

The default models are public:
- âœ… No account needed
- âœ… No token required
- âœ… Ready to use out of the box

```bash
docker-compose up --build
# Models automatically download from HuggingFace
```

### Option B: Upload Your Own Models

If you have custom-trained Kronos weights:

1. **Install HuggingFace CLI**:
```bash
pip install huggingface_hub
huggingface-cli login
```

2. **Upload Tokenizer**:
```python
from kronos_integration.model import KronosTokenizer

# Load your trained tokenizer
tokenizer = KronosTokenizer.from_pretrained('./path/to/your/tokenizer')

# Push to HuggingFace
tokenizer.push_to_hub('YOUR_USERNAME/kronos-tokenizer-custom')
```

3. **Upload Model**:
```python
from kronos_integration.model import Kronos

# Load your trained model
model = Kronos.from_pretrained('./path/to/your/model')

# Push to HuggingFace
model.push_to_hub('YOUR_USERNAME/kronos-model-custom')
```

4. **Update Environment Variables**:
```bash
KRONOS_MODEL_ID=YOUR_USERNAME/kronos-model-custom
KRONOS_TOKENIZER_ID=YOUR_USERNAME/kronos-tokenizer-custom
```

### Option C: Private Models

For private/proprietary models:

1. **Create HuggingFace token**: https://huggingface.co/settings/tokens

2. **Set token in environment**:
```bash
export HF_TOKEN=hf_xxxxxxxxxxxxx
```

3. **Update docker-compose.yml**:
```yaml
services:
  ai-service:
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - KRONOS_MODEL_ID=YOUR_ORG/private-kronos-model
```

---

## ğŸ§ª Testing

### Test Prediction Endpoint

```bash
# Using curl
curl -X POST http://localhost:5001/predict \
  -H "Content-Type: application/json" \
  -d '{"isin": "US67066G1040", "horizon_days": 5}'

# Using test script
./test-api.sh
```

### Expected Response (Kronos Mode)

```json
{
  "isin": "US67066G1040",
  "model": "kronos",
  "model_version": "NeoQuasar/Kronos-small",
  "prediction_horizon_days": 5,
  "current_price": 100.0,
  "predictions": [
    {
      "day": 1,
      "predicted_price": 101.23,
      "confidence": 0.85
    },
    ...
  ],
  "metadata": {
    "device": "cpu",
    "max_context": 512,
    "lookback_periods": 400,
    "prediction_periods": 390
  }
}
```

---

## ğŸ”§ Troubleshooting

### Issue: Model Download Fails

```
Error: Failed to download model from NeoQuasar/Kronos-small
```

**Solutions:**
1. Check internet connection
2. Verify model ID exists on HuggingFace
3. For private models, ensure `HF_TOKEN` is set correctly
4. Try with `KRONOS_MODE=simulated` to use simulator

### Issue: Out of Memory

```
RuntimeError: CUDA out of memory
```

**Solutions:**
1. Use CPU: `KRONOS_DEVICE=cpu`
2. Reduce context: `KRONOS_MAX_CONTEXT=256`
3. Use smaller model: `KRONOS_MODEL_ID=NeoQuasar/Kronos-mini`

### Issue: Slow First Startup

**Expected behavior**: First startup downloads ~200MB of model weights (1-5 min).

**Solutions:**
1. Persist cache: Mount volume `-v ./cache:/root/.cache/huggingface`
2. Pre-download models in Dockerfile (see Advanced section below)
3. Use `KRONOS_MODE=simulated` for development

---

## ğŸš€ Advanced: Production Deployment

### 1. Pre-download Models in Docker Image

```dockerfile
# ai-service/Dockerfile
FROM python:3.12-slim

# ... existing setup ...

# Pre-download models during build (no runtime download)
RUN python -c "
from huggingface_hub import snapshot_download
snapshot_download('NeoQuasar/Kronos-small')
snapshot_download('NeoQuasar/Kronos-Tokenizer-base')
"

COPY . .
CMD ["python", "main.py"]
```

### 2. Use MLflow Model Registry

For enterprise deployments, consider MLflow:

```python
# Alternative: MLflow registry
import mlflow

model_uri = "models:/kronos-production/latest"
model = mlflow.pytorch.load_model(model_uri)
```

### 3. GPU Deployment

```yaml
# docker-compose.gpu.yml
services:
  ai-service:
    environment:
      - KRONOS_DEVICE=cuda:0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

---

## ğŸ“š Further Reading

- [HuggingFace Hub Documentation](https://huggingface.co/docs/hub/index)
- [PyTorch Model Hub Mixin](https://huggingface.co/docs/huggingface_hub/package_reference/mixins)
- [Kronos Model Paper](ref/Kronos/README.md)
- [Model Registry Best Practices](https://ml-ops.org/content/model-registry)

---

## ğŸ”„ Migration Path

### Current State (Simulator)
```python
KRONOS_MODE=simulated  # Statistical simulator
```

### Testing Real Model
```python
KRONOS_MODE=auto       # Try Kronos, fallback to simulator
```

### Production with Real Model
```python
KRONOS_MODE=kronos     # Require Kronos (fail if unavailable)
HF_TOKEN=hf_xxx        # For private models
```

---

## ğŸ¤ Contributing

To add new model versions:

1. Train model using `ref/Kronos/finetune/`
2. Push to HuggingFace: `model.push_to_hub('YOUR_USERNAME/kronos-v2')`
3. Update environment: `KRONOS_MODEL_ID=YOUR_USERNAME/kronos-v2`
4. Test with `KRONOS_MODE=kronos`
5. Update this README with new model ID
